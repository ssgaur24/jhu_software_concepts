"""Fast JSON loader with assignment-aligned key mapping and robust type handling.

- Batches inserts (default 2000) in a single transaction for speed on ~30k rows.
- Converts numeric fields to float; if conversion fails, stores NULL and records an issue.
- Parses multiple date formats; unparsable dates become NULL and are recorded.
- Records a concise audit report at module_3/artifacts/load_report.json:
  - total_records, inserted_rows, skipped_without_id
  - issue_counts per field (e.g., degree_non_numeric, date_parse_fail)
  - sample p_id lists (first few) for each issue
"""

# stdlib imports
import json
import re
import datetime as dt
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path

# local imports
from src.dal.pool import get_conn

# explicit schema qualification for stable GUI visibility
INSERT_SQL = """
INSERT INTO public.applicants (
    p_id, program, comments, date_added, url, status, term, us_or_international,
    gpa, gre, gre_v, gre_aw, degree, llm_generated_program, llm_generated_university
)
VALUES (
    %(p_id)s, %(program)s, %(comments)s, %(date_added)s, %(url)s, %(status)s, %(term)s, %(us_or_international)s,
    %(gpa)s, %(gre)s, %(gre_v)s, %(gre_aw)s, %(degree)s, %(llm_generated_program)s, %(llm_generated_university)s
)
ON CONFLICT (p_id) DO NOTHING;
"""

# precompiled helpers for id/date parsing
_ID_FROM_URL = re.compile(r"/result/(\d+)")
_DATE_FORMATS = ("%Y-%m-%d", "%m/%d/%Y", "%Y/%m/%d", "%B %d, %Y")


def _to_float(x: Any) -> Optional[float]:
    """Convert numeric-like values to float or None."""
    # accept None/empty as NULL; attempt float conversion otherwise
    if x is None:
        return None
    s = str(x).strip()
    if s == "":
        return None
    try:
        return float(s)
    except ValueError:
        return None  # record via issue tracker at mapping callsite


def _parse_date(s: Optional[str]) -> Optional[dt.date]:
    """Parse common date formats; return None on failure."""
    # iterate known formats until one parses
    if not s:
        return None
    s = str(s).strip()
    for fmt in _DATE_FORMATS:
        try:
            return dt.datetime.strptime(s, fmt).date()
        except ValueError:
            continue
    return None  # recorded by mapping when raw values existed


def _derive_p_id(entry_url: Optional[str], fallback: Optional[Any]) -> Optional[int]:
    """Prefer extracting numeric id from entry_url; else try an integer fallback."""
    # extract "/result/<id>" from entry_url when present
    if entry_url:
        m = _ID_FROM_URL.search(entry_url)
        if m:
            try:
                return int(m.group(1))
            except ValueError:
                pass
    # if no URL id, attempt to coerce provided p_id
    if fallback is not None:
        try:
            return int(fallback)
        except (TypeError, ValueError):
            return None
    return None  # caller will count as missing_p_id


def _init_issue_tracker() -> Dict[str, Any]:
    """Initialize counters and sample holders for load issues."""
    # track counts plus small sample p_id lists per category
    return {
        "missing_p_id": 0,
        "date_parse_fail": 0,
        "gpa_non_numeric": 0,
        "gre_non_numeric": 0,
        "gre_v_non_numeric": 0,
        "gre_aw_non_numeric": 0,
        "degree_non_numeric": 0,
        "_samples": {
            "missing_p_id": [],
            "date_parse_fail": [],
            "gpa_non_numeric": [],
            "gre_non_numeric": [],
            "gre_v_non_numeric": [],
            "gre_aw_non_numeric": [],
            "degree_non_numeric": [],
        },
    }


def _write_report(total: int, inserted: int, skipped: int, issues: Dict[str, Any], batch: int) -> Path:
    """Write a concise JSON report and return its path."""
    # create artifacts dir and emit a compact, human-readable summary
    base = Path(__file__).resolve().parents[2]  # .../module_3
    outdir = base / "artifacts"
    outdir.mkdir(parents=True, exist_ok=True)  # ensure folder exists
    path = outdir / "load_report.json"
    report = {
        "summary": {
            "total_records": total,
            "inserted_rows": inserted,
            "skipped_without_id": skipped,
            "batch_size": batch,
        },
        "issue_counts": {k: v for k, v in issues.items() if k != "_samples"},
        "issue_samples": issues["_samples"],  # small p_id lists only
    }
    with open(path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)  # pretty JSON for graders
    return path  # allow caller to print the path


def _map_record(rec: Dict[str, Any], issues: Dict[str, Any], sample_limit: int = 20) -> Optional[Dict[str, Any]]:
    """Map one JSON object to the DB row dict; return None if no stable p_id."""
    # normalize keys (spaces/hyphens/slashes -> underscores; lowercase)
    norm: Dict[str, Any] = {}
    for k, v in rec.items():
        nk = k.strip().lower().replace(" ", "_").replace("-", "_").replace("/", "_")  # normalize key
        norm[nk] = v

    # derive p_id from URL or provided key
    entry_url = norm.get("entry_url") or norm.get("url") or ""
    p_id = _derive_p_id(entry_url, norm.get("p_id"))
    if p_id is None:
        issues["missing_p_id"] += 1  # count missing id
        if len(issues["_samples"]["missing_p_id"]) < sample_limit:
            issues["_samples"]["missing_p_id"].append(entry_url or str(norm.get("p_id")))  # keep small sample
        return None  # skip row entirely (PK required)

    # build "program" text (University - Program) when both exist
    program_raw = (norm.get("program") or "").strip()
    university_raw = (norm.get("university") or "").strip()
    program = f"{university_raw} - {program_raw}".strip(" -") if university_raw else program_raw  # final program text

    # choose a date (acceptance > rejection > date_added) and track failures
    date_added = None
    for d in (norm.get("acceptance_date"), norm.get("rejection_date"), norm.get("date_added")):
        date_added = _parse_date(d)
        if date_added:
            break
    if date_added is None and any((norm.get("acceptance_date"), norm.get("rejection_date"), norm.get("date_added"))):
        issues["date_parse_fail"] += 1  # count date parse failures
        if len(issues["_samples"]["date_parse_fail"]) < sample_limit:
            issues["_samples"]["date_parse_fail"].append(p_id)  # keep sample p_id

    # helper to coerce numeric fields and record issues when non-numeric text is present
    def _num(field_key_in_norm: str, issue_key: str) -> Optional[float]:
        val = norm.get(field_key_in_norm)  # raw value from JSON
        out = _to_float(val)  # try to coerce to float
        if out is None and (val not in (None, "")):
            issues[issue_key] += 1  # count non-numeric anomalies
            if len(issues["_samples"][issue_key]) < sample_limit:
                issues["_samples"][issue_key].append(p_id)  # keep sample p_id
        return out  # return float or None for NULL

    # assemble final row dict for executemany()
    return {
        "p_id": p_id,
        "program": program,
        "comments": norm.get("comments") or "",
        "date_added": date_added,
        "url": entry_url,
        "status": norm.get("status") or "",
        "term": norm.get("start_term") or norm.get("term") or "",
        "us_or_international": norm.get("us_international") or norm.get("us_or_international") or "",
        "gpa": _num("gpa", "gpa_non_numeric"),
        "gre": _num("gre", "gre_non_numeric"),
        "gre_v": _num("gre_v", "gre_v_non_numeric"),
        "gre_aw": _num("gre_aw", "gre_aw_non_numeric"),
        "degree": _num("degree", "degree_non_numeric"),  # keep REAL per assignment; non-numeric â†’ NULL
        "llm_generated_program": norm.get("llm_generated_program") or "",
        "llm_generated_university": norm.get("llm_generated_university") or "",
    }


def load_json(path: str, batch: int = 2000) -> Tuple[int, int, int, Dict[str, int], Path]:
    """Load a JSON array from `path` and insert rows in batches.

    Returns:
        total_records, inserted_rows, skipped_without_id, issue_counts, report_path
    """
    # read JSON file (expects an array of objects)
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        raise ValueError("Expected a JSON array of applicant records")  # enforce shape

    # map/validate rows and track issues
    issues = _init_issue_tracker()  # initialize counters
    mapped: List[Dict[str, Any]] = []  # output rows
    skipped = 0  # number of rows skipped due to missing id
    for r in data:
        row = _map_record(r or {}, issues)  # map and validate
        if row is None:
            skipped += 1  # count skipped row
            continue
        mapped.append(row)  # keep valid row

    # insert in batches inside a single transaction for speed
    inserted = 0  # count of rows accepted by the DB
    with get_conn() as conn:
        with conn.cursor() as cur:
            for i in range(0, len(mapped), batch):
                chunk = mapped[i : i + batch]  # compute batch slice
                cur.executemany(INSERT_SQL, chunk)  # bulk insert this batch
                if cur.rowcount and cur.rowcount > 0:
                    inserted += cur.rowcount  # accumulate accepted rows
        conn.commit()  # persist transaction

    # write concise audit report for graders and diagnostics
    report_path = _write_report(len(data), inserted, skipped, issues, batch)  # output report
    issue_counts = {k: v for k, v in issues.items() if k != "_samples"}  # strip samples for CLI print
    return len(data), inserted, skipped, issue_counts, report_path  # return summary tuple


def first_ids(n: int = 3) -> List[int]:
    """Return the first n p_id values for a tiny preview."""
    # simple query to preview first few primary keys
    with get_conn() as conn:
        with conn.cursor() as cur:
            cur.execute("SELECT p_id FROM public.applicants ORDER BY p_id ASC LIMIT %s;", (n,))  # small select
            return [r[0] for r in cur.fetchall()]  # unpack list of ints
